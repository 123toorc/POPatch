diff -uprN 4.14.79/arch/x86/cpu/.gitignore 4.14.79_popcorn/arch/x86/cpu/.gitignore
--- 4.14.79/arch/x86/cpu/.gitignore	1969-12-31 19:00:00.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/cpu/.gitignore	2020-05-24 22:41:27.734914867 -0400
@@ -0,0 +1 @@
+capflags.c
diff -uprN 4.14.79/arch/x86/cpu/proc.c 4.14.79_popcorn/arch/x86/cpu/proc.c
--- 4.14.79/arch/x86/cpu/proc.c	1969-12-31 19:00:00.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/cpu/proc.c	2020-05-24 22:41:27.738914855 -0400
@@ -0,0 +1,298 @@
+// SPDX-License-Identifier: GPL-2.0
+#include <linux/smp.h>
+#include <linux/timex.h>
+#include <linux/string.h>
+#include <linux/seq_file.h>
+#include <linux/cpufreq.h>
+
+#include "cpu.h"
+
+#include <popcorn/bundle.h>
+
+extern void send_remote_cpu_info_request(unsigned int nid);
+extern unsigned int get_number_cpus_from_remote_node(unsigned int nid);
+extern int remote_proc_cpu_info(struct seq_file *m, unsigned int nid,
+				unsigned int vpos);
+
+static struct cpu_global_info {
+	unsigned int remote;
+	struct cpuinfo_x86 *c;
+	unsigned int vpos;
+	unsigned int nid;
+} cpu_global_info;
+
+static struct cpuinfo_x86 c;
+
+/*
+ * num_cpus: # of cores of each nodes
+ * num_total_cpus: # of total cpus of all connected nodes
+ */
+static unsigned int num_cpus[MAX_POPCORN_NODES];
+static unsigned int num_total_cpus;
+
+/*
+ *	Get CPU information for use by the procfs.
+ */
+static void show_cpuinfo_core(struct seq_file *m, struct cpuinfo_x86 *c,
+			      unsigned int cpu)
+{
+#ifdef CONFIG_SMP
+	seq_printf(m, "physical id\t: %d\n", c->phys_proc_id);
+	seq_printf(m, "siblings\t: %d\n",
+		   cpumask_weight(topology_core_cpumask(cpu)));
+	seq_printf(m, "core id\t\t: %d\n", c->cpu_core_id);
+	seq_printf(m, "cpu cores\t: %d\n", c->booted_cores);
+	seq_printf(m, "apicid\t\t: %d\n", c->apicid);
+	seq_printf(m, "initial apicid\t: %d\n", c->initial_apicid);
+#endif
+}
+
+#ifdef CONFIG_X86_32
+static void show_cpuinfo_misc(struct seq_file *m, struct cpuinfo_x86 *c)
+{
+	seq_printf(m,
+		   "fdiv_bug\t: %s\n"
+		   "f00f_bug\t: %s\n"
+		   "coma_bug\t: %s\n"
+		   "fpu\t\t: %s\n"
+		   "fpu_exception\t: %s\n"
+		   "cpuid level\t: %d\n"
+		   "wp\t\t: yes\n",
+		   static_cpu_has_bug(X86_BUG_FDIV) ? "yes" : "no",
+		   static_cpu_has_bug(X86_BUG_F00F) ? "yes" : "no",
+		   static_cpu_has_bug(X86_BUG_COMA) ? "yes" : "no",
+		   static_cpu_has(X86_FEATURE_FPU) ? "yes" : "no",
+		   static_cpu_has(X86_FEATURE_FPU) ? "yes" : "no",
+		   c->cpuid_level);
+}
+#else
+static void show_cpuinfo_misc(struct seq_file *m, struct cpuinfo_x86 *c)
+{
+	seq_printf(m,
+		   "fpu\t\t: yes\n"
+		   "fpu_exception\t: yes\n"
+		   "cpuid level\t: %d\n"
+		   "wp\t\t: yes\n",
+		   c->cpuid_level);
+}
+#endif
+
+static int show_cpuinfo(struct seq_file *m, void *v)
+{
+	struct cpuinfo_x86 *c = v;
+	unsigned int cpu;
+	int i;
+
+	cpu = c->cpu_index;
+	seq_printf(m, "processor\t: %u\n"
+		   "vendor_id\t: %s\n"
+		   "cpu family\t: %d\n"
+		   "model\t\t: %u\n"
+		   "model name\t: %s\n",
+		   cpu,
+		   c->x86_vendor_id[0] ? c->x86_vendor_id : "unknown",
+		   c->x86,
+		   c->x86_model,
+		   c->x86_model_id[0] ? c->x86_model_id : "unknown");
+
+	if (c->x86_stepping || c->cpuid_level >= 0)
+		seq_printf(m, "stepping\t: %d\n", c->x86_stepping);
+	else
+		seq_puts(m, "stepping\t: unknown\n");
+	if (c->microcode)
+		seq_printf(m, "microcode\t: 0x%x\n", c->microcode);
+
+	if (cpu_has(c, X86_FEATURE_TSC)) {
+		unsigned int freq = aperfmperf_get_khz(cpu);
+
+		if (!freq)
+			freq = cpufreq_quick_get(cpu);
+		if (!freq)
+			freq = cpu_khz;
+		seq_printf(m, "cpu MHz\t\t: %u.%03u\n",
+			   freq / 1000, (freq % 1000));
+	}
+
+	/* Cache size */
+	if (c->x86_cache_size)
+		seq_printf(m, "cache size\t: %u KB\n", c->x86_cache_size);
+
+	show_cpuinfo_core(m, c, cpu);
+	show_cpuinfo_misc(m, c);
+
+	seq_puts(m, "flags\t\t:");
+	for (i = 0; i < 32*NCAPINTS; i++)
+		if (cpu_has(c, i) && x86_cap_flags[i] != NULL)
+			seq_printf(m, " %s", x86_cap_flags[i]);
+
+	seq_puts(m, "\nbugs\t\t:");
+	for (i = 0; i < 32*NBUGINTS; i++) {
+		unsigned int bug_bit = 32*NCAPINTS + i;
+
+		if (cpu_has_bug(c, bug_bit) && x86_bug_flags[i])
+			seq_printf(m, " %s", x86_bug_flags[i]);
+	}
+
+	seq_printf(m, "\nbogomips\t: %lu.%02lu\n",
+		   c->loops_per_jiffy/(500000/HZ),
+		   (c->loops_per_jiffy/(5000/HZ)) % 100);
+
+#ifdef CONFIG_X86_64
+	if (c->x86_tlbsize > 0)
+		seq_printf(m, "TLB size\t: %d 4K pages\n", c->x86_tlbsize);
+#endif
+	seq_printf(m, "clflush size\t: %u\n", c->x86_clflush_size);
+	seq_printf(m, "cache_alignment\t: %d\n", c->x86_cache_alignment);
+	seq_printf(m, "address sizes\t: %u bits physical, %u bits virtual\n",
+		   c->x86_phys_bits, c->x86_virt_bits);
+
+	seq_puts(m, "power management:");
+	for (i = 0; i < 32; i++) {
+		if (c->x86_power & (1 << i)) {
+			if (i < ARRAY_SIZE(x86_power_flags) &&
+			    x86_power_flags[i])
+				seq_printf(m, "%s%s",
+					   x86_power_flags[i][0] ? " " : "",
+					   x86_power_flags[i]);
+			else
+				seq_printf(m, " [%d]", i);
+		}
+	}
+
+	seq_puts(m, "\n\n");
+
+	return 0;
+}
+
+static void calc_nid_vpos(loff_t *pos, unsigned int *pnid, unsigned int *vpos)
+{
+	int i = 0;
+
+	*pnid = 0;
+	*vpos = 0;
+
+	for (i = nr_cpu_ids; i <= num_total_cpus; i++) {
+		if ((*pnid) == my_nid)
+			(*pnid)++;
+
+		if ((*vpos) == num_cpus[*pnid]) {
+			*vpos = 0;
+			(*pnid)++;
+		}
+
+		if (i == (*pos))
+			break;
+
+		(*vpos)++;
+	}
+}
+
+static void *c_start(struct seq_file *m, loff_t *pos)
+{
+	unsigned int vpos = 0;
+	unsigned int nid = 0;
+
+	if (my_nid == -1)
+		goto local;
+
+	if ((*pos) < nr_cpu_ids) {
+		goto local;
+	} else if ((*pos) == nr_cpu_ids) {
+		int i = 0;
+		int j = 0;
+		bool connected = false;
+
+		/* Check the connection with remote nodes */
+		for (i = 0; i < MAX_POPCORN_NODES; i++) {
+			if (get_popcorn_node_online(i)) {
+				connected = true;
+				break;
+			}
+		}
+
+		if (connected == false) {
+			/* No connection */
+			goto local;
+		} else {
+			/* Connection with remote nodes */
+			for (i = 0; i < MAX_POPCORN_NODES; i++) {
+				if (i == my_nid) {
+					num_cpus[i] = nr_cpu_ids;
+					j = j + nr_cpu_ids;
+					continue;
+				}
+				if (get_popcorn_node_online(i)) {
+					send_remote_cpu_info_request(i);
+					num_cpus[i] = get_number_cpus_from_remote_node(i);
+					j = j + num_cpus[i];
+				} else {
+					num_cpus[i] = 0;
+				}
+			}
+
+			num_total_cpus = j;
+			goto remote;
+		}
+	} else if ((*pos) > nr_cpu_ids) {
+		goto remote;
+	}
+
+local:
+	*pos = cpumask_next(*pos - 1, cpu_online_mask);
+	if ((*pos) < nr_cpu_ids) {
+		cpu_global_info.remote = 0;
+		c = cpu_data(*pos);
+		cpu_global_info.c = &c;
+
+		return &cpu_global_info;
+	}
+
+	return NULL;
+
+remote:
+	if ((*pos) < num_total_cpus) {
+		calc_nid_vpos(pos, &nid, &vpos);
+
+		cpu_global_info.remote = 1;
+		cpu_global_info.vpos = vpos;
+		cpu_global_info.nid = nid;
+
+		return &cpu_global_info;
+	}
+
+	return NULL;
+}
+
+static void *c_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	(*pos)++;
+	return c_start(m, pos);
+}
+
+static void c_stop(struct seq_file *m, void *v)
+{
+}
+
+static int c_show(struct seq_file *m, void *v)
+{
+	struct cpu_global_info *cpu_global_info = v;
+	struct cpuinfo_x86 *c;
+
+	if (cpu_global_info->remote == 1) {
+		remote_proc_cpu_info(m,
+			cpu_global_info->nid,
+			cpu_global_info->vpos);
+	} else {
+		c = cpu_global_info->c;
+		show_cpuinfo(m, c);
+	}
+
+	return 0;
+}
+
+const struct seq_operations cpuinfo_op = {
+	.start	= c_start,
+	.next	= c_next,
+	.stop	= c_stop,
+	.show	= c_show,
+};
diff -uprN 4.14.79/arch/x86/entry/syscalls/syscall_64.tbl 4.14.79_popcorn/arch/x86/entry/syscalls/syscall_64.tbl
--- 4.14.79/arch/x86/entry/syscalls/syscall_64.tbl	2018-11-04 08:52:51.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/entry/syscalls/syscall_64.tbl	2020-05-24 22:41:27.706914955 -0400
@@ -339,7 +339,10 @@
 330	common	pkey_alloc		sys_pkey_alloc
 331	common	pkey_free		sys_pkey_free
 332	common	statx			sys_statx
-
+340 64	popcorn_migrate			sys_popcorn_migrate
+341 64	popcorn_propose_migration	sys_popcorn_propose_migration
+342 64	popcorn_get_thread_status	sys_popcorn_get_thread_status
+343 64	popcorn_get_node_info		sys_popcorn_get_node_info
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
 # for native 64-bit operation.
diff -uprN 4.14.79/arch/x86/.gitignore 4.14.79_popcorn/arch/x86/.gitignore
--- 4.14.79/arch/x86/.gitignore	2018-11-04 08:52:51.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/.gitignore	2020-05-24 22:41:27.694914992 -0400
@@ -3,4 +3,3 @@ tools/test_get_len
 tools/insn_sanity
 purgatory/kexec-purgatory.c
 purgatory/purgatory.ro
-
diff -uprN 4.14.79/arch/x86/kernel/cpuinfo.c 4.14.79_popcorn/arch/x86/kernel/cpuinfo.c
--- 4.14.79/arch/x86/kernel/cpuinfo.c	1969-12-31 19:00:00.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/kernel/cpuinfo.c	2020-05-24 22:41:27.738914855 -0400
@@ -0,0 +1,142 @@
+/*
+ * File:
+  * popcorn_cpuinfo_arch.c
+ *
+ * Description:
+ * 	this file provides the architecture specific functionality of
+  * populating cpuinfo
+ *
+ * Created on:
+ * 	Oct 10, 2014
+ *
+ * Author:
+ * 	Sharath Kumar Bhat, SSRG, VirginiaTech
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/cpu.h>
+#include <linux/slab.h>
+#include <linux/timex.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+
+#include <linux/cpufreq.h>
+
+#include <popcorn/cpuinfo.h>
+
+static void *remote_c_start(loff_t *pos)
+{
+	if (*pos == 0) /* just in case, cpu 0 is not the first */
+		*pos = cpumask_first(cpu_online_mask);
+	else {
+		*pos = cpumask_next(*pos - 1, cpu_online_mask);
+	}
+
+	if ((*pos) < nr_cpu_ids)
+		return &cpu_data(*pos);
+	return NULL;
+}
+
+int fill_cpu_info(struct remote_cpu_info *res)
+{
+	loff_t pos = 0;
+	struct cpuinfo_x86 *c;
+	unsigned int cpu = 0;
+	int i, count = 0;
+	struct cpuinfo_arch_x86 *arch = &res->x86;
+
+	res->arch_type = POPCORN_ARCH_X86;
+
+	while (count < NR_CPUS) {
+		void *p = remote_c_start(&pos);
+		struct percore_info_x86 *core = &arch->cores[count];
+
+		if(p == NULL)
+			break;
+
+		c = p;
+		pos++;
+
+#ifdef CONFIG_SMP
+		cpu = c->cpu_index;
+#endif
+		core->processor = cpu;
+		strcpy(core->vendor_id,
+				c->x86_vendor_id[0] ? c->x86_vendor_id : "unknown");
+		core->cpu_family = c->x86;
+		core->model = c->x86_model;
+		strcpy(core->model_name,
+				c->x86_model_id[0] ? c->x86_model_id : "unknown");
+
+		if (c->x86_stepping || c->cpuid_level >= 0)
+			core->stepping = c->x86_stepping;
+		else
+			core->stepping = -1;
+
+		if (c->microcode)
+			core->microcode = c->microcode;
+
+		if (cpu_has(c, X86_FEATURE_TSC)) {
+			unsigned int freq = cpufreq_quick_get(cpu);
+
+			if (!freq)
+				freq = cpu_khz;
+			core->cpu_freq = freq / 1000;
+		}
+
+		/* Cache size */
+		if (c->x86_cache_size >= 0)
+			core->cache_size = c->x86_cache_size;
+
+		strcpy(core->fpu, "yes");
+		strcpy(core->fpu_exception, "yes");
+		core->cpuid_level = c->cpuid_level;
+		strcpy(core->wp, "yes");
+
+		strcpy(core->flags, "");
+		for (i = 0; i < 32 * NCAPINTS; i++)
+			if (cpu_has(c, i) && x86_cap_flags[i] != NULL){
+				strcat(core->flags, x86_cap_flags[i]);
+				strcat(core->flags, " ");
+			}
+
+		core->nbogomips = c->loops_per_jiffy / (500000 / HZ);
+
+#ifdef CONFIG_X86_64
+		if (c->x86_tlbsize > 0)
+			core->TLB_size = c->x86_tlbsize;
+#endif
+		core->clflush_size = c->x86_clflush_size;
+		core->cache_alignment = c->x86_cache_alignment;
+		core->bits_physical = c->x86_phys_bits;
+		core->bits_virtual = c->x86_virt_bits;
+
+		strcpy(core->power_management, "");
+		for (i = 0; i < 32; i++) {
+			if (c->x86_power & (1 << i)) {
+				if (i < ARRAY_SIZE(x86_power_flags) && x86_power_flags[i])
+					strcat(core->flags,
+							x86_power_flags[i][0] ? " " : "");
+			}
+		}
+
+		count++;
+	}
+	arch->num_cpus = count;
+
+	return 0;
+}
+
+int get_proccessor_id(void)
+{
+	unsigned int a, b, feat;
+
+	asm volatile(
+			 "cpuid"							// call cpuid
+			 : "=a" (a), "=b" (b), "=d" (feat)	// outputs
+			 : "0" (1)							// inputs
+			 : "cx" );
+
+	return !(feat & (1 << 25));
+}
diff -uprN 4.14.79/arch/x86/kernel/process_server.c 4.14.79_popcorn/arch/x86/kernel/process_server.c
--- 4.14.79/arch/x86/kernel/process_server.c	1969-12-31 19:00:00.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/kernel/process_server.c	2020-05-24 22:41:27.746914830 -0400
@@ -0,0 +1,266 @@
+/*
+ * File:
+ * 	process_server.c
+ *
+ * Description:
+ * 	this file implements the x86 architecture specific
+ *  helper functionality of the process server
+ *
+ * Created on:
+ * 	Sep 19, 2014
+ *
+ * Author:
+ * 	Sharath Kumar Bhat, SSRG, VirginiaTech
+ *
+ */
+
+/* File includes */
+#include <linux/sched.h>
+#include <linux/kdebug.h>
+#include <linux/ptrace.h>
+#include <asm/uaccess.h>
+#include <asm/prctl.h>
+#include <asm/proto.h>
+#include <asm/desc.h>
+#include <asm/fpu/internal.h>
+
+#include <popcorn/types.h>
+#include <popcorn/regset.h>
+
+/*
+ * Function:
+ *		save_thread_info
+ *
+ * Description:
+ *		this function saves the architecture specific info of the task
+ *		to the struct struct field_arch structure passed
+ *
+ * Input:
+ *	regs,	pointer to the pt_regs field of the task
+ *
+ * Output:
+ *	arch,	pointer to the struct field_arch structure type where the
+ *			architecture specific information of the task has to be
+ *			saved
+ *
+ * Return value:
+ *	on success, returns 0
+ * 	on failure, returns negative integer
+ */
+int save_thread_info(struct field_arch *arch)
+{
+	unsigned short fsindex, gsindex;
+	unsigned long ds, es, fs, gs;
+	int cpu;
+
+	BUG_ON(!arch);
+
+	cpu = get_cpu();
+
+	/*
+	 * Segments
+	 * CS and SS are set during the user/kernel mode switch.
+	 * Thus, nothing to do with them.
+	 */
+
+	ds = current->thread.ds;
+	es = current->thread.es;
+
+	savesegment(fs, fsindex);
+	if (fsindex) {
+		//fs = get_desc_base(current->thread.tls_array + 0);
+		fs = current->thread.fsbase;
+		//directly get the fs base
+	} else {
+		rdmsrl(MSR_FS_BASE, fs);
+	}
+
+	savesegment(gs, gsindex);
+	if (gsindex) {
+		//gs = get_desc_base(current->thread.tls_array + GS_TLS);
+		gs = current->thread.gsbase;
+		//directly get the gs base
+	} else {
+		rdmsrl(MSR_KERNEL_GS_BASE, gs);
+	}
+
+	WARN_ON(ds);
+	WARN_ON(es);
+	WARN_ON(gs);
+	arch->tls = fs;
+	arch->fpu_active = !!current->thread.fpu.initialized; //rename
+
+	put_cpu();
+
+	/*
+	PSPRINTK("%s [%d] tls %lx\n", __func__, current->pid, arch->tls);
+	PSPRINTK("%s [%d] fpu %sactive\n", __func__, current->pid,
+			arch->fpu_active ? "" : "in");
+	*/
+
+	return 0;
+}
+
+
+/*
+ * Function:
+ *		restore_thread_info
+ *
+ * Description:
+ *		this function restores the architecture specific info of the
+ *		task from the struct field_arch structure passed
+ *
+ * Input:
+ * 	arch,	pointer to the struct field_arch structure type from which the
+ *			architecture specific information of the task has to be
+ *			restored
+ *
+ *	restore_segments,
+ *			restore segmentations as well if segs is true. Unless, do
+ *			not restore the segmentation units (for back migration)
+ *
+ * Output:
+ *	none
+ *
+ * Return value:
+ *	on success, returns 0
+ * 	on failure, returns negative integer
+ */
+int restore_thread_info(struct field_arch *arch, bool restore_segments)
+{
+	struct pt_regs *regs = current_pt_regs();
+	struct regset_x86_64 *regset = &arch->regs_x86;
+	int cpu;
+
+	cpu = get_cpu();
+
+	regs->r15 = regset->r15;
+	regs->r14 = regset->r14;
+	regs->r13 = regset->r13;
+	regs->r12 = regset->r12;
+	regs->bp = regset->rbp;
+	regs->bx = regset->rbx;
+
+	regs->r11 = regset->r11;
+	regs->r10 = regset->r10;
+	regs->r9 = regset->r9;
+	regs->r8 = regset->r8;
+	regs->ax = regset->rax;
+	regs->cx = regset->rcx;
+	regs->dx = regset->rdx;
+	regs->si = regset->rsi;
+	regs->di = regset->rdi;
+
+	regs->ip = regset->rip;
+	regs->sp = regset->rsp;
+	regs->flags = regset->rflags;
+
+	if (restore_segments) {
+		regs->cs = __USER_CS;
+		regs->ss = __USER_DS;
+
+		/*
+		current->thread.ds = regset->ds;
+		current->thread.es = regset->es;
+		*/
+
+		if (arch->tls) {
+			do_arch_prctl_64(current, ARCH_SET_FS, arch->tls);
+			//function rename
+		}
+		/*
+		if (arch->thread_gs) {
+			do_arch_prctl(current, ARCH_SET_GS, arch->thread_gs);
+		}
+		*/
+		if (arch->fpu_active) {
+			fpu__initialize(&current->thread.fpu);
+			//function rename
+		}
+	}
+
+	put_cpu();
+
+#ifdef CONFIG_POPCORN_DEBUG_VERBOSE
+	PSPRINTK("%s [%d] ip %lx\n", __func__, current->pid,
+			regs->ip);
+	PSPRINTK("%s [%d] sp %lx bp %lx\n", __func__, current->pid,
+			regs->sp, regs->bp);
+	PSPRINTK("%s [%d] fs %lx fpu %sactive\n", __func__, current->pid,
+			arch->tls, arch->fpu_active ? "" : "in");
+#endif
+	return 0;
+}
+
+#include <asm/stacktrace.h>
+noinline_for_stack void update_frame_pointer(void)
+{
+	unsigned long *rbp;
+	//get_bp(rbp); /* update_frame_pointer */
+	asm("movq %%rbp, %0" : "=r" (rbp) :);
+	/* User rbp is at one stack frames below */
+	*rbp = current_pt_regs()->bp;	/* sched_migrate */
+}
+
+
+/*
+ * Function:
+ *		dump_processor_regs
+ *
+ * Description:
+ *		this function prints the architecture specific registers specified
+ *		in the input argument
+ *
+ * Input:
+ * 	task,	pointer to the architecture specific registers
+ *
+ * Output:
+ * 	none
+ *
+ * Return value:
+ *	void
+ *
+ * Why don't use show_all() for x86?
+ */
+void dump_processor_regs(struct pt_regs* regs)
+{
+	unsigned long fs, gs;
+	unsigned long fsindex, gsindex;
+
+	dump_stack();
+	if (!regs) return;
+	printk(KERN_ALERT"DUMP REGS %s\n", __func__);
+
+	if (NULL != regs) {
+		printk(KERN_ALERT"r15{%lx}\n",regs->r15);
+		printk(KERN_ALERT"r14{%lx}\n",regs->r14);
+		printk(KERN_ALERT"r13{%lx}\n",regs->r13);
+		printk(KERN_ALERT"r12{%lx}\n",regs->r12);
+		printk(KERN_ALERT"r11{%lx}\n",regs->r11);
+		printk(KERN_ALERT"r10{%lx}\n",regs->r10);
+		printk(KERN_ALERT"r9{%lx}\n",regs->r9);
+		printk(KERN_ALERT"r8{%lx}\n",regs->r8);
+		printk(KERN_ALERT"bp{%lx}\n",regs->bp);
+		printk(KERN_ALERT"bx{%lx}\n",regs->bx);
+		printk(KERN_ALERT"ax{%lx}\n",regs->ax);
+		printk(KERN_ALERT"cx{%lx}\n",regs->cx);
+		printk(KERN_ALERT"dx{%lx}\n",regs->dx);
+		printk(KERN_ALERT"di{%lx}\n",regs->di);
+		printk(KERN_ALERT"orig_ax{%lx}\n",regs->orig_ax);
+		printk(KERN_ALERT"ip{%lx}\n",regs->ip);
+		printk(KERN_ALERT"cs{%lx}\n",regs->cs);
+		printk(KERN_ALERT"flags{%lx}\n",regs->flags);
+		printk(KERN_ALERT"sp{%lx}\n",regs->sp);
+		printk(KERN_ALERT"ss{%lx}\n",regs->ss);
+	}
+	rdmsrl(MSR_FS_BASE, fs);
+	rdmsrl(MSR_GS_BASE, gs);
+	printk(KERN_ALERT"fs{%lx} - %lx content %lx\n",fs, current->thread.fsbase, fs ? * (unsigned long*)fs : 0x1234567l);
+	printk(KERN_ALERT"gs{%lx} - %lx content %lx\n",gs, current->thread.gsbase, fs ? * (unsigned long*)gs : 0x1234567l);
+
+	savesegment(fs, fsindex);
+	savesegment(gs, gsindex);
+	printk(KERN_ALERT"fsindex{%lx} - %x\n",fsindex, current->thread.fsindex);
+	printk(KERN_ALERT"gsindex{%lx} - %x\n",gsindex, current->thread.gsindex);
+	printk(KERN_ALERT"REGS DUMP COMPLETE\n");
+}
diff -uprN 4.14.79/arch/x86/mm/fault.c 4.14.79_popcorn/arch/x86/mm/fault.c
--- 4.14.79/arch/x86/mm/fault.c	2018-11-04 08:52:51.000000000 -0500
+++ 4.14.79_popcorn/arch/x86/mm/fault.c	2020-05-24 22:41:27.762914781 -0400
@@ -29,6 +29,11 @@
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
+#ifdef CONFIG_POPCORN
+#include <popcorn/types.h>
+#include <popcorn/vma_server.h>
+#endif
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1369,6 +1374,19 @@ retry:
 	}
 
 	vma = find_vma(mm, address);
+#ifdef CONFIG_POPCORN
+	/* vma worker should not fault */
+	BUG_ON(tsk->is_worker);
+
+	if (distributed_remote_process(tsk)) {
+		if (!vma || vma->vm_start > address) {
+			if (vma_server_fetch_vma(tsk, address) == 0) {
+				/* Replace with updated VMA */
+				vma = find_vma(mm, address);
+			}
+		}
+	}
+#endif
 	if (unlikely(!vma)) {
 		bad_area(regs, error_code, address);
 		return;
